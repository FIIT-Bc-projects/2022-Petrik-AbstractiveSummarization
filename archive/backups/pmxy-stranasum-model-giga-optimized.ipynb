{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sebastian Petrik - Stranasum - Simple transformer abstractive summarization\n\nInspired by:\n- https://www.kaggle.com/code/ashishsingh226/text-summarization-with-transformers\n- https://www.tensorflow.org/text/tutorials/transformer\n","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade -q wandb --quiet\n!pip install evaluate rouge-score contractions --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.environ.get('KAGGLE_CONTAINER_NAME')) # check if kaggle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\nsorted(list(filter(\n    lambda x: x[0] in ['numpy', 'pandas', 'tensorflow', 'tensorflow-text', 'keras', 'tensorflow-estimator', 'tensorflow-datasets', 'wandb', 'evaluate', 'rouge_score', 'gensim'],\n    [(i.key, i.version) for i in pkg_resources.working_set]\n)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nimport os\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport operator as op\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom pprint import pprint\nimport evaluate\nimport gensim\nimport gensim.downloader\nfrom tqdm import tqdm\nimport contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration and Wandb","metadata":{}},{"cell_type":"code","source":"# Setup seeds\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1' \nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    # Meta\n    wandb_project = 'stranasum-tune',\n    wandb_group = 'gigaword',\n    host = 'kaggle',\n    \n    # Data - use dataset name with dataset directory\n    # dataset_name = \"pmxy-stranasum-data/xsum_10-150_3-40_v0.05_t0.05\",\n    # dataset_name = \"pmxy-stranasum-data/inshorts_10-70_3-16_v0.05_t0.05\",\n    # dataset_name = \"pmxy-stranasum-data-combined/combined_10-150_3-40_v0.05_t0.05\",\n    dataset_name = \"stranasum-gigaword-70-to-25/gigaword\",\n    \n    # Sequences\n    # - define lengths according to data\n    max_input_length = 70, # Encoder sequence length, max article len, max token count in Encoder.\n    max_target_length = 25, # Decoder sequence length, max summary len, max token count in Decoder.\n    \n    # Embeddings - remember to set correct d_model !!!\n    embedding_mode = 'glove6b100d', # normal, glove840b300d, glove6b100d and alternatives\n    embeddings_trainable = False, # only for pretrained embeddings\n\n    # Transformer hyperparameters\n    num_layers = 3, # 4\n    d_model = 100, # 128 -> embedding length ~ dimensions\n    dff = 512, # 512\n    num_heads = 8, # 8\n    dropout_rate = 0.1, # 0.1\n\n    # Training\n    early_stopping_patience = 2, # patience - num of non-improving consecutive epochs\n    max_epochs = 3, # 40\n    batch_size = 512, # 256\n    learning_rate_warmup_steps = 4000 # 4000\n)\n\nprint(\"Config:\")\npprint(CONFIG)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb.login(key=user_secrets.get_secret(\"wandb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading","metadata":{}},{"cell_type":"markdown","source":"- load data from preprocessing","metadata":{}},{"cell_type":"code","source":"# show available data\n!ls ../input\n!ls ../input/pmxy-stranasum-preprocessing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir = \"../input/\"\ndf_train = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_train.csv\")\ndf_val = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_val.csv\")\ndf_test = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_test.csv\")\n\nprint(\"Train:\", df_train.shape)\nprint(\"Val:\", df_val.shape)\nprint(\"Test:\", df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# Custom tokenizer\nclass Tokenizer(tf.Module):\n    \n    def __init__(self, vectorization_dataset: tf.data.Dataset, max_length: int):\n        super().__init__(name=\"Tokenizer\")\n        \n        self.max_length = max_length\n        \n        # Create and fit vectorizer from dataset\n        self.vectorizer = tf.keras.layers.TextVectorization(\n            output_mode='int',\n            output_sequence_length=self.max_length,\n            standardize=None\n        )\n        self.vectorizer.adapt(vectorization_dataset.batch(1024))\n        \n        # conversions\n        self.word_to_id = tf.keras.layers.StringLookup(\n            vocabulary=self.vectorizer.get_vocabulary(),\n            mask_token='', oov_token='[UNK]'\n        )\n        self.id_to_word = tf.keras.layers.StringLookup(\n            vocabulary=self.vectorizer.get_vocabulary(),\n            mask_token='', oov_token='[UNK]',\n            invert=True\n        )\n        \n        # attributes\n        self.vocab_size = self.vectorizer.vocabulary_size()\n        self.start_token = self.word_to_id('<sos>')\n        self.end_token = self.word_to_id('<eos>')\n        \n        print(f\"Tokenizer maxlen={self.max_length}, top vocabulary: {self.vectorizer.get_vocabulary()[:10]}\")\n        \n        \n    # to convert text to tokens, call vectorizer directly !\n        \n    # convert tokens back to text\n    @tf.function\n    def tokens_to_text(self, tokens):\n        words = self.id_to_word(tokens)\n        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n        result = tf.strings.regex_replace(result, '^ *<sos> *', '')\n        result = tf.strings.regex_replace(result, ' *<eos> *$', '')\n        result = tf.strings.regex_replace(result, '<dot>', '.')\n        return result\n\n# Setup tokenizers\n\ninput_tokenizer = Tokenizer(\n    tf.data.Dataset.from_tensor_slices(df_train['article']),\n    CONFIG['max_input_length']\n)\n\ntarget_tokenizer = Tokenizer(\n    tf.data.Dataset.from_tensor_slices(df_train['summary']),\n    CONFIG['max_target_length']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try out \nsample_tokens = input_tokenizer.vectorizer(tf.constant(['<sos> the dog ate the food <eos>']))\nsample_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenizer.tokens_to_text(sample_tokens).numpy().astype('str') # binary string tensor into decoded string array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_tokenizer.vectorizer(tf.constant('<sos> a man was murdered <eos>'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graph function for vectorization and shifted label creation\n# x: Tensor (batch, 2)\n# for each batch we have input and target string in the tensor\ndef transform_to_tokenized_sequences(x: tf.Tensor):\n    # take batch of input strings, vectorize them\n    inputs = input_tokenizer.vectorizer(x[:,0])\n    \n    # take batch of output strings, vectorize them\n    targets = target_tokenizer.vectorizer(x[:,1])\n    \n    # targets are present as variable length (Ragged) Tensors per batch member ...\n    \n    # drop their EOS tokens at the end\n    targets_inputs = targets[:,:-1]\n    \n    # drop SOS token, shifting sequence 1 step behind providing next word labels for each step\n    targets_labels = targets[:,1:]\n    \n    # final output feedable to the Transformer\n    return (inputs, targets_inputs), targets_labels","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:32:54.871081Z","iopub.execute_input":"2023-05-01T20:32:54.871452Z","iopub.status.idle":"2023-05-01T20:32:54.879002Z","shell.execute_reply.started":"2023-05-01T20:32:54.871420Z","shell.execute_reply":"2023-05-01T20:32:54.877476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply tokenization and create batched tf datasets ...\n\n# convert articles and summaries into tensorflow dataset\n# shuffle - shuffle with buffer size = size of data for full uniform shuffle\n# attach a mapper to map texts into token tensors with shifted target labels\n\ndataset_train = tf.data.Dataset.from_tensor_slices(\n    # convert_to_tokenized_sequences(df_train['article'], df_train['summary'])\n    df_train[['article', 'summary']],\n).shuffle(df_train.shape[0]).batch(CONFIG['batch_size']).map(\n    transform_to_tokenized_sequences\n)\n\ndataset_val = tf.data.Dataset.from_tensor_slices(\n    # convert_to_tokenized_sequences(df_val['article'], df_val['summary'])\n    df_val[['article', 'summary']],\n).shuffle(df_val.shape[0]).batch(CONFIG['batch_size']).map(\n    transform_to_tokenized_sequences\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:11.671142Z","iopub.execute_input":"2023-05-01T20:34:11.671898Z","iopub.status.idle":"2023-05-01T20:34:18.875871Z","shell.execute_reply.started":"2023-05-01T20:34:11.671858Z","shell.execute_reply":"2023-05-01T20:34:18.867102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take 1 sample batch for further inspection (later)\nfor (sample_input, sample_target), sample_target_labels in dataset_train.take(1):\n    break\n    \nprint('Sample single batch from dataset:')\n\nprint(sample_input.shape)\nprint(sample_input[0])\nprint(sample_target.shape)\nprint(sample_target_labels.shape)\n\nprint(sample_target[0])\nprint(sample_target_labels[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:22.802786Z","iopub.execute_input":"2023-05-01T20:34:22.803490Z","iopub.status.idle":"2023-05-01T20:34:26.956789Z","shell.execute_reply.started":"2023-05-01T20:34:22.803451Z","shell.execute_reply":"2023-05-01T20:34:26.955618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try to get special token numbers so we can see them in prints\nprint(input_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> [UNK] <dot> <pad>')))\nprint(target_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> [UNK] <dot> <pad>')))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:28.318150Z","iopub.execute_input":"2023-05-01T20:34:28.318918Z","iopub.status.idle":"2023-05-01T20:34:28.383692Z","shell.execute_reply.started":"2023-05-01T20:34:28.318877Z","shell.execute_reply":"2023-05-01T20:34:28.381793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained embeddings","metadata":{}},{"cell_type":"code","source":"# (Legacy)\n# glove_vectors = gensim.downloader.load('glove-twitter-25')\n# glove_vectors.most_similar('queen')\n# glove_vectors.vectors.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:30.270911Z","iopub.execute_input":"2023-05-01T20:34:30.271678Z","iopub.status.idle":"2023-05-01T20:34:30.276003Z","shell.execute_reply.started":"2023-05-01T20:34:30.271626Z","shell.execute_reply":"2023-05-01T20:34:30.274853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_emb_matrix(tokenizer, embedding_vectors):\n    hits = 0\n    misses = 0\n    embedding_dim = CONFIG['d_model']\n\n    tokenizer_vocabulary = tokenizer.vectorizer.get_vocabulary()\n    embedding_matrix = np.zeros((tokenizer.vocab_size, embedding_dim))\n\n    for i in tqdm(range(tokenizer.vocab_size)):\n        embedding_val = embedding_vectors.get(tokenizer_vocabulary[i])\n\n        if embedding_val is not None:\n            embedding_matrix[i] = embedding_val\n            hits += 1\n        else:\n            misses += 1\n\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n\n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:31.033705Z","iopub.execute_input":"2023-05-01T20:34:31.034806Z","iopub.status.idle":"2023-05-01T20:34:31.043772Z","shell.execute_reply.started":"2023-05-01T20:34:31.034763Z","shell.execute_reply":"2023-05-01T20:34:31.042425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_emb_matrix = None\ntarget_emb_matrix = None\n\nif CONFIG['embedding_mode'] != 'normal':\n    print(\"Setting up glove pretrained embeddings...\")\n    \n    embedding_vectors = {}\n    \n    path = None\n    if CONFIG['embedding_mode'] == 'glove840b300d':\n        path = '../input/glove840b300dtxt/glove.840B.300d.txt'\n    elif CONFIG['embedding_mode'] == 'glove6b50d':\n        path = '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt'\n    elif CONFIG['embedding_mode'] == 'glove6b100d':\n        path = '../input/nlpword2vecembeddingspretrained/glove.6B.100d.txt'\n    elif CONFIG['embedding_mode'] == 'glove6b200d':\n        path = '../input/nlpword2vecembeddingspretrained/glove.6B.200d.txt'\n    \n    # load glove\n    f = open(path)\n    for line in tqdm(f):\n        value = line.split(' ')\n        word = value[0]\n        coef = np.array(value[1:],dtype = 'float32')\n        embedding_vectors[word] = coef\n        \n    # create matrices for tokenizers\n    input_emb_matrix = create_emb_matrix(input_tokenizer, embedding_vectors)\n    target_emb_matrix = create_emb_matrix(target_tokenizer, embedding_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:31.464126Z","iopub.execute_input":"2023-05-01T20:34:31.465323Z","iopub.status.idle":"2023-05-01T20:34:42.965055Z","shell.execute_reply.started":"2023-05-01T20:34:31.465274Z","shell.execute_reply":"2023-05-01T20:34:42.963868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer model implementation","metadata":{}},{"cell_type":"code","source":"def positional_encoding(length, depth):\n    depth = depth/2\n\n    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n\n    angle_rates = 1 / (10000**depths)         # (1, depth)\n    angle_rads = positions * angle_rates      # (pos, depth)\n\n    pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1) \n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n# We try out the encoding func\npos_encoding = positional_encoding(length=2048, depth=512)\nprint('Positional encoding shape', pos_encoding.shape) # Check the shape.\n\n# Plot the dimensions.\nplt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:47.235758Z","iopub.execute_input":"2023-05-01T20:34:47.236521Z","iopub.status.idle":"2023-05-01T20:34:48.797953Z","shell.execute_reply.started":"2023-05-01T20:34:47.236480Z","shell.execute_reply":"2023-05-01T20:34:48.796658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEmbedding(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, d_model, emb_matrix):\n        super().__init__()\n        self.d_model = d_model\n        \n        if emb_matrix is None:\n            self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n        else:\n            self.embedding = tf.keras.layers.Embedding(\n            vocab_size,\n            d_model,\n            weights=[emb_matrix],\n            trainable=CONFIG['embeddings_trainable'],\n            mask_zero=True\n        )\n        \n        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n    def compute_mask(self, *args, **kwargs):\n        return self.embedding.compute_mask(*args, **kwargs)\n\n    def call(self, x):\n        length = tf.shape(x)[1]\n        x = self.embedding(x)\n        \n        # This factor sets the relative scale of the embedding and positonal_encoding.\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x = x + self.pos_encoding[tf.newaxis, :length, :]\n        return x \n\n# Try\n# sample_emb_input = TransformerEmbedding(vocab_size=input_tokenizer.vocab_size, d_model=512)(sample_input)\n# sample_emb_target = TransformerEmbedding(vocab_size=target_tokenizer.vocab_size, d_model=512)(sample_target)\n\n# sample_emb_target._keras_mask","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:51.061211Z","iopub.execute_input":"2023-05-01T20:34:51.061610Z","iopub.status.idle":"2023-05-01T20:34:51.071823Z","shell.execute_reply.started":"2023-05-01T20:34:51.061574Z","shell.execute_reply":"2023-05-01T20:34:51.070531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base attention for further subclassing\nclass BaseAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n        self.layernorm = tf.keras.layers.LayerNormalization()\n        self.add = tf.keras.layers.Add()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:52.002695Z","iopub.execute_input":"2023-05-01T20:34:52.003477Z","iopub.status.idle":"2023-05-01T20:34:52.010453Z","shell.execute_reply.started":"2023-05-01T20:34:52.003433Z","shell.execute_reply":"2023-05-01T20:34:52.008991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrossAttention(BaseAttention):\n    \n    # x = target sequence, context = context sequence\n    def call(self, x, context):\n        attn_output, attn_scores = self.mha(\n            query=x,\n            key=context,\n            value=context,\n            return_attention_scores=True)\n\n        # Cache the attention scores for plotting later.\n        self.last_attn_scores = attn_scores\n\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n\n        return x\n\n# Try on sample\n# sample_ca = CrossAttention(num_heads=2, key_dim=512)\n# print(sample_emb_input.shape)\n# print(sample_emb_target.shape)\n# print(sample_ca(sample_emb_input, sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:52.588375Z","iopub.execute_input":"2023-05-01T20:34:52.588756Z","iopub.status.idle":"2023-05-01T20:34:52.596263Z","shell.execute_reply.started":"2023-05-01T20:34:52.588721Z","shell.execute_reply":"2023-05-01T20:34:52.594867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GlobalSelfAttention(BaseAttention):\n    def call(self, x):\n        attn_output = self.mha(\n            query=x,\n            value=x,\n            key=x)\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n        return x\n\n# Try\n# sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n# print(sample_emb_input.shape)\n# print(sample_gsa(sample_emb_input).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:53.266744Z","iopub.execute_input":"2023-05-01T20:34:53.267826Z","iopub.status.idle":"2023-05-01T20:34:53.275358Z","shell.execute_reply.started":"2023-05-01T20:34:53.267783Z","shell.execute_reply":"2023-05-01T20:34:53.273806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(BaseAttention):\n    def call(self, x):\n        attn_output = self.mha(\n            query=x,\n            value=x,\n            key=x,\n            use_causal_mask = True)\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n        return x\n    \n# sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n# print(sample_emb_target.shape)\n# print(sample_csa(sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:53.945524Z","iopub.execute_input":"2023-05-01T20:34:53.948353Z","iopub.status.idle":"2023-05-01T20:34:53.957560Z","shell.execute_reply.started":"2023-05-01T20:34:53.948304Z","shell.execute_reply":"2023-05-01T20:34:53.956384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test latter elements do not depend on earlier elements, making no difference\n# if we remove the earlier elements before or after applying csa layer\n# out1 = sample_csa(embed_target(sample_target[:, :3])) \n# out2 = sample_csa(embed_target(sample_target))[:, :3]\n# tf.reduce_max(abs(out1 - out2)).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:54.950362Z","iopub.execute_input":"2023-05-01T20:34:54.951417Z","iopub.status.idle":"2023-05-01T20:34:54.956488Z","shell.execute_reply.started":"2023-05-01T20:34:54.951365Z","shell.execute_reply":"2023-05-01T20:34:54.955414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(tf.keras.layers.Layer):\n    def __init__(self, d_model, dff, dropout_rate=0.1):\n        super().__init__()\n        self.seq = tf.keras.Sequential([\n          tf.keras.layers.Dense(dff, activation='relu'),\n          tf.keras.layers.Dense(d_model),\n          tf.keras.layers.Dropout(dropout_rate)\n        ])\n        self.add = tf.keras.layers.Add()\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n\n    def call(self, x):\n        x = self.add([x, self.seq(x)])\n        x = self.layer_norm(x) \n        return x\n\n# Try\n# sample_ffn = FeedForward(512, 2048)\n# print(sample_emb_target.shape)\n# print(sample_ffn(sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:55.832690Z","iopub.execute_input":"2023-05-01T20:34:55.833436Z","iopub.status.idle":"2023-05-01T20:34:55.841815Z","shell.execute_reply.started":"2023-05-01T20:34:55.833396Z","shell.execute_reply":"2023-05-01T20:34:55.840459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n        super().__init__()\n\n        self.self_attention = GlobalSelfAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.ffn = FeedForward(d_model, dff)\n\n    def call(self, x):\n        x = self.self_attention(x)\n        x = self.ffn(x)\n        return x\n    \n# sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n# print(sample_encoder_layer(sample_emb_input).shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:56.300841Z","iopub.execute_input":"2023-05-01T20:34:56.302181Z","iopub.status.idle":"2023-05-01T20:34:56.310337Z","shell.execute_reply.started":"2023-05-01T20:34:56.302126Z","shell.execute_reply":"2023-05-01T20:34:56.309079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, *, num_layers, d_model, num_heads,\n                   dff, vocab_size, dropout_rate=0.1):\n        \n        super().__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.pos_embedding = TransformerEmbedding(\n            vocab_size=vocab_size, d_model=d_model,\n            emb_matrix=input_emb_matrix,\n        )\n\n        self.enc_layers = [\n            EncoderLayer(d_model=d_model,\n                         num_heads=num_heads,\n                         dff=dff,\n                         dropout_rate=dropout_rate)\n            for _ in range(num_layers)\n        ]\n        \n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, x):\n        # `x` is token-IDs shape: (batch_size, seq_len)\n        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n        # Add dropout.\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x)\n\n        return x  # Shape `(batch_size, seq_len, d_model)`.\n\n# Try\n# sample_encoder = Encoder(num_layers=4,\n#                          d_model=512,\n#                          num_heads=8,\n#                          dff=2048,\n#                          vocab_size=input_tokenizer.vocab_size)\n\n# sample_encoder_output = sample_encoder(sample_input, training=False)\n\n# print(sample_input.shape)\n# print(sample_encoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:56.733792Z","iopub.execute_input":"2023-05-01T20:34:56.734487Z","iopub.status.idle":"2023-05-01T20:34:56.744269Z","shell.execute_reply.started":"2023-05-01T20:34:56.734449Z","shell.execute_reply":"2023-05-01T20:34:56.743102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    \n    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.causal_self_attention = CausalSelfAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.cross_attention = CrossAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.ffn = FeedForward(d_model, dff)\n\n    def call(self, x, context):\n        x = self.causal_self_attention(x=x)\n        x = self.cross_attention(x=x, context=context)\n\n        # Cache the last attention scores for plotting later\n        self.last_attn_scores = self.cross_attention.last_attn_scores\n\n        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n        return x\n    \n# sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n# sample_decoder_layer_output = sample_decoder_layer(\n#     x=sample_emb_target, context=sample_emb_input\n# )\n\n# print(sample_emb_input.shape)\n# print(sample_emb_target.shape)\n# print(sample_decoder_layer_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:57.112866Z","iopub.execute_input":"2023-05-01T20:34:57.114209Z","iopub.status.idle":"2023-05-01T20:34:57.123097Z","shell.execute_reply.started":"2023-05-01T20:34:57.114160Z","shell.execute_reply":"2023-05-01T20:34:57.121875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.pos_embedding = TransformerEmbedding(vocab_size=vocab_size,\n                                                 d_model=d_model,\n                                                 emb_matrix=target_emb_matrix\n                                                )\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.dec_layers = [\n            DecoderLayer(d_model=d_model, num_heads=num_heads,\n                         dff=dff, dropout_rate=dropout_rate)\n            for _ in range(num_layers)]\n\n        self.last_attn_scores = None\n\n    def call(self, x, context):\n        # `x` is token-IDs shape (batch, target_seq_len)\n        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x  = self.dec_layers[i](x, context)\n\n        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n        # The shape of x is (batch_size, target_seq_len, d_model).\n        return x\n\n# Try\n# sample_decoder = Decoder(num_layers=4,\n#                          d_model=512,\n#                          num_heads=8,\n#                          dff=2048,\n#                          vocab_size=target_tokenizer.vocab_size)\n\n# sample_decoder_output = sample_decoder(x=sample_target, context=sample_emb_input)\n\n# print(sample_target.shape)\n# print(sample_emb_input.shape)\n# print(sample_decoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:57.471111Z","iopub.execute_input":"2023-05-01T20:34:57.471901Z","iopub.status.idle":"2023-05-01T20:34:57.482413Z","shell.execute_reply.started":"2023-05-01T20:34:57.471861Z","shell.execute_reply":"2023-05-01T20:34:57.480721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_decoder.last_attn_scores.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:58.054735Z","iopub.execute_input":"2023-05-01T20:34:58.055545Z","iopub.status.idle":"2023-05-01T20:34:58.060724Z","shell.execute_reply.started":"2023-05-01T20:34:58.055503Z","shell.execute_reply":"2023-05-01T20:34:58.059278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n        \n        super().__init__()\n        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               vocab_size=input_vocab_size,\n                               dropout_rate=dropout_rate)\n\n        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               vocab_size=target_vocab_size,\n                               dropout_rate=dropout_rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inputs):\n        # To use a Keras model with `.fit` you must pass all your inputs in the\n        # first argument.\n        context, x  = inputs\n\n        context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n        # Final linear layer output.\n        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n        try:\n            # Drop the keras mask, so it doesn't scale the losses/metrics.\n            # b/250038731\n            del logits._keras_mask\n        except AttributeError:\n            pass\n\n        # Return the final output and the attention weights.\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:58.283002Z","iopub.execute_input":"2023-05-01T20:34:58.283404Z","iopub.status.idle":"2023-05-01T20:34:58.295252Z","shell.execute_reply.started":"2023-05-01T20:34:58.283366Z","shell.execute_reply":"2023-05-01T20:34:58.294018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct transformer\ntransformer = Transformer(\n    num_layers=CONFIG['num_layers'],\n    d_model=CONFIG['d_model'],\n    num_heads=CONFIG['num_heads'],\n    dff=CONFIG['dff'],\n    input_vocab_size=input_tokenizer.vocab_size,\n    target_vocab_size=target_tokenizer.vocab_size,\n    dropout_rate=CONFIG['dropout_rate']\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:34:58.981731Z","iopub.execute_input":"2023-05-01T20:34:58.982734Z","iopub.status.idle":"2023-05-01T20:34:59.105833Z","shell.execute_reply.started":"2023-05-01T20:34:58.982693Z","shell.execute_reply":"2023-05-01T20:34:59.104725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenizer.vocab_size, target_tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:01.232088Z","iopub.execute_input":"2023-05-01T20:35:01.233308Z","iopub.status.idle":"2023-05-01T20:35:01.241761Z","shell.execute_reply.started":"2023-05-01T20:35:01.233258Z","shell.execute_reply":"2023-05-01T20:35:01.240181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call transformer on sample input, this will build it and setup inputs\n\nsample_transformer_output = transformer((sample_input, sample_target))\nprint(sample_input.shape)\nprint(sample_target.shape)\nprint(sample_transformer_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:02.839359Z","iopub.execute_input":"2023-05-01T20:35:02.840073Z","iopub.status.idle":"2023-05-01T20:35:07.397134Z","shell.execute_reply.started":"2023-05-01T20:35:02.840032Z","shell.execute_reply":"2023-05-01T20:35:07.395073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_transformer_attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n# print(sample_transformer_attn_scores.shape) ","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:09.646558Z","iopub.execute_input":"2023-05-01T20:35:09.647811Z","iopub.status.idle":"2023-05-01T20:35:09.654233Z","shell.execute_reply.started":"2023-05-01T20:35:09.647758Z","shell.execute_reply":"2023-05-01T20:35:09.653071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:11.359900Z","iopub.execute_input":"2023-05-01T20:35:11.360718Z","iopub.status.idle":"2023-05-01T20:35:11.408027Z","shell.execute_reply.started":"2023-05-01T20:35:11.360633Z","shell.execute_reply":"2023-05-01T20:35:11.406839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps):\n        super().__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \n    def get_config(self):\n        config = {\n            'd_model': self.d_model,\n            'warmup_steps': self.warmup_steps\n        }\n        return config\n#         base_config = super(CustomSchedule, self).get_config()\n#         return dict(list(base_config.items()) + list(config.items()))\n\nlearning_rate = CustomSchedule(CONFIG['d_model'], CONFIG['learning_rate_warmup_steps'])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\n\nplt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\nplt.ylabel('Learning Rate')\nplt.xlabel('Train Step')","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:16.867017Z","iopub.execute_input":"2023-05-01T20:35:16.867803Z","iopub.status.idle":"2023-05-01T20:35:17.289194Z","shell.execute_reply.started":"2023-05-01T20:35:16.867765Z","shell.execute_reply":"2023-05-01T20:35:17.287999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_loss(label, pred):\n    mask = label != 0\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none'\n    )\n    loss = loss_object(label, pred)\n\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss *= mask\n\n    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n    \n    return loss\n\n\ndef masked_accuracy(label, pred):\n    pred = tf.argmax(pred, axis=2)\n    label = tf.cast(label, pred.dtype)\n    match = label == pred\n\n    mask = label != 0\n\n    match = match & mask\n\n    match = tf.cast(match, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n\n\n# masked_loss(tf.constant([0.5], dtype=tf.float32), tf.constant([[0.5, 0.2]], dtype=tf.float32))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:19.300298Z","iopub.execute_input":"2023-05-01T20:35:19.301343Z","iopub.status.idle":"2023-05-01T20:35:19.311130Z","shell.execute_reply.started":"2023-05-01T20:35:19.301286Z","shell.execute_reply":"2023-05-01T20:35:19.309932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy]\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:20.172503Z","iopub.execute_input":"2023-05-01T20:35:20.173299Z","iopub.status.idle":"2023-05-01T20:35:20.194477Z","shell.execute_reply.started":"2023-05-01T20:35:20.173256Z","shell.execute_reply":"2023-05-01T20:35:20.193104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Callbacks \n!mkdir -p checkpoints\n\nmodeldir = f\"checkpoints\"\ncheckpoint_filepath = modeldir + '/checkpoint.hdf'\nprint('Model checkpoint:', checkpoint_filepath)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode='min', verbose=1,\n    patience=CONFIG['early_stopping_patience'],\n    restore_best_weights=True # restore only best weights relative to val_loss\n)\n\ncsv_logger=tf.keras.callbacks.CSVLogger(\n    modeldir + '/log.csv', separator=\",\", append=True\n)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True\n)\n\ntry:\n    model.load_weights(checkpoint_filepath)\n    print('Loaded model weights checkpoint.')\nexcept:\n    print('Cannot load model weights from checkpoint, it may not exist yet.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb_run = wandb.init(\n    project=CONFIG['wandb_project'], \n    config=CONFIG,\n    group=CONFIG['wandb_group'], \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Training\n    \nhistory = transformer.fit(\n    dataset_train,\n    epochs=CONFIG['max_epochs'],\n    validation_data=dataset_val,\n    callbacks=[\n        WandbCallback(save_model=False),\n        early_stopping,\n        csv_logger,\n        model_checkpoint_callback\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T20:35:28.343199Z","iopub.execute_input":"2023-05-01T20:35:28.344652Z","iopub.status.idle":"2023-05-01T20:35:28.393507Z","shell.execute_reply.started":"2023-05-01T20:35:28.344580Z","shell.execute_reply":"2023-05-01T20:35:28.392530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    fig, (axl, axa) = plt.subplots(nrows=2, ncols=1)\n    axl.plot(history.history['loss'], label='loss')\n    axl.plot(history.history['val_loss'], label='val_loss')\n    axl.set_ylim([0, 10])\n    axl.set_xlabel('Epoch')\n    axl.set_ylabel('Loss')\n    axl.legend()\n    axl.grid(True)\n    \n    axa.plot(history.history['masked_accuracy'], label='masked_accuracy')\n    axa.plot(history.history['val_masked_accuracy'], label='val_masked_accuracy')\n    axa.set_ylim([0, 1])\n    axa.set_xlabel('Epoch')\n    axa.set_ylabel('Accuracy')\n    axa.legend()\n    axa.grid(True)\n    \n    fig.show()\n    \nif history != None:\n    plot_history(history)\nelse:\n    print(\"No history to display.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# LEGACY - unoptimized inference using python array\nclass SummarizerSingleStep(tf.Module):\n    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n        \n        # todo\n        self.transformer = transformer\n        self.input_tokenizer = input_tokenizer\n        self.target_tokenizer = target_tokenizer\n\n    # expect sentence to be prepared with <sos> <eos> and clean\n    # @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n    def __call__(self, sentence: str):\n\n        encoder_input = tf.expand_dims(self.input_tokenizer.vectorizer(sentence), 0)\n\n        # `tf.TensorArray` is required here (instead of a Python list), so that the\n        # dynamic-loop can be traced by `tf.function`.\n        # output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        # output_array = output_array.write(0, summary_start_token)\n        \n        summary_start_token = target_tokenizer.vectorizer('<sos>')[0].numpy()\n        summary_end_token = target_tokenizer.vectorizer('<eos>')[0].numpy()\n        \n        decoder_input = [summary_start_token]\n        output = tf.expand_dims(decoder_input, 0)\n\n        for i in tf.range(self.target_tokenizer.max_length):\n            \n            predictions = self.transformer([encoder_input, output], training=False)\n\n            # Select the last token from the `seq_len` dimension.\n            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n            # TODO: why cast needed here ??\n            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), dtype=tf.int32)\n\n            # Concatenate the `predicted_id` to the output which is given to the\n            # decoder as its input.\n            # output_array = output_array.write(i+1, predicted_id[0])\n            output = tf.concat([output, predicted_id], axis=-1)\n\n            if predicted_id == summary_end_token:\n                break\n        \n        \n        prediction = tf.squeeze(output, axis=0)\n        tokens = np.expand_dims(prediction.numpy(), 0)\n        \n        # print(tokens)\n        \n        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n        text = self.target_tokenizer.tokens_to_text(tokens)[0]\n\n        # `tf.function` prevents us from using the attention_weights that were\n        # calculated on the last iteration of the loop.\n        # So, recalculate them outside the loop.\n        self.transformer([encoder_input, output[:,:-1]], training=False)\n        attention_weights = self.transformer.decoder.last_attn_scores\n\n        return text, tokens, attention_weights\n    \n# def str_to_tf(self, text: str):\n#         return tf.constant([text])\n    \n# def str_from_tf(self, tensor: tf.Tensor):\n#     return bytes.decode(tensor.numpy())\n\n# Optimized inference module using tensorflow tensorarray, saveable as TF graph saved model\nclass SummarizerModule(tf.Module):\n    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n        \n        self.transformer = transformer\n        self.input_tokenizer = input_tokenizer\n        self.target_tokenizer = target_tokenizer\n\n    # TF func inference prediction\n    # expect sentence to be prepared with <sos> <eos> and clean\n    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n    def predict(self, sentence: tf.Tensor):\n\n        encoder_input = self.input_tokenizer.vectorizer(sentence)\n        \n        start_token = target_tokenizer.vectorizer(tf.constant('<sos>'))[0][tf.newaxis]\n        end_token = target_tokenizer.vectorizer(tf.constant('<eos>'))[0][tf.newaxis]\n        \n        # `tf.TensorArray` is required here (instead of a Python list), so that the\n        # dynamic-loop can be traced by `tf.function`.\n        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        output_array = output_array.write(0, start_token)\n        output = tf.transpose(output_array.stack())\n\n        for i in tf.range(self.target_tokenizer.max_length):\n            \n            output = tf.transpose(output_array.stack())\n            \n            predictions = self.transformer([encoder_input, output], training=False)\n\n            # Select the last token from the `seq_len` dimension.\n            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n            # argmax\n            predicted_id = tf.argmax(predictions, axis=-1)\n\n            # Concatenate the `predicted_id` to the output which is given to the\n            # decoder as its input.\n            # output_array = output_array.write(i+1, predicted_id[0])\n            output_array = output_array.write(i+1, predicted_id[0])\n\n            # stop on end\n            if predicted_id == end_token:\n                break\n        \n        output = tf.transpose(output_array.stack())\n        \n        # print(tokens)\n        \n        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n        text = self.target_tokenizer.tokens_to_text(output)[0]\n\n        # `tf.function` prevents us from using the attention_weights that were\n        # calculated on the last iteration of the loop.\n        # So, recalculate them outside the loop.\n        self.transformer([encoder_input, output[:,:-1]], training=False)\n        attention_weights = self.transformer.decoder.last_attn_scores\n\n        return text, output, attention_weights\n\nsummarizer_module = SummarizerModule(transformer, input_tokenizer, target_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# (inference benchmark)\n\n# summarizer_single = SummarizerSingleStep(transformer, input_tokenizer, target_tokenizer)\n# df_train[:5]['article'].apply(lambda text: summarizer_single(text)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# df_train[:5]['article'].apply(lambda text: summarizer(summarizer.to_tf(text))[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_tokens(text):\n        text = text.lower()\n        text = text.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n        text = text.replace(\"<unk>\", \"##\")\n        text = text.replace(\"<dot>\", \". \") # normal syntax with dot at end\n        text = text.strip()\n        return text\n\ndef remove_special_tokens_frame(frame: pd.DataFrame):\n    frame['article'] = frame['article'].apply(remove_special_tokens)\n    frame['summary'] = frame['summary'].apply(remove_special_tokens)\n    frame['predicted'] = frame['predicted'].apply(remove_special_tokens)\n    return frame\n    \n# Run summarization inference on entire frame\ndef summarize_frame(frame):\n    \n    frame = frame.copy()\n    frame['predicted'] = '<NONE>'\n    \n    for i in range(0, frame.shape[0]):\n        if i%25 == 0:\n            print(f\"Summarising ... {i}/{frame.shape[0]}\")\n            \n        article = frame.iloc[i]['article']\n        summary = frame.iloc[i]['summary']\n        \n        summarized_tf, summarized_tokens, attention_weights = summarizer_module.predict(\n            tf.constant([article])\n        )\n        \n        summarized_text = bytes.decode(summarized_tf.numpy())\n        \n        frame.iloc[i, frame.columns.get_loc('predicted')] = summarized_text\n        \n    return frame\n\ndef pretty_summaries(frame):\n    \n    for i, row in frame.iterrows():\n        print(f\"\\n ------------------\")\n        print(f\"Article  : {remove_special_tokens(row['article'])}\")\n        print(f\"\\nSummary  : {remove_special_tokens(row['summary'])}\")\n        print(f\"\\nPredicted: {remove_special_tokens(row['predicted'])}\")\n        print()\n        print(f\"------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text processing module from pmxy-stranasum-preprocessing used as summarizer module component\nclass TextProcessor:\n\n    def clean_text(self, text: str):\n        text = str(text).lower()\n        text = re.sub(r\"&.[1-9]+;\",\" \", str(text))\n        text=re.sub(\"(\\\\t)\", ' ', str(text))\n        text=re.sub(\"(\\\\r)\", ' ', str(text))\n        text=re.sub(\"(\\\\n)\", ' ', str(text))\n        text=re.sub(\"(__+)\", ' ', str(text))\n        text=re.sub(\"(--+)\", ' ', str(text))\n        text=re.sub(\"(~~+)\", ' ', str(text))\n        text=re.sub(\"(\\+\\++)\", ' ', str(text))\n        text=re.sub(\"(\\.\\.+)\", ' ', str(text))\n        \n        # fix contractions to base form\n        text = contractions.fix(text)\n\n        #remove special tokens <>()|&©ø\"',;?~*!\n        text=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n        # CNN mail data cleanup\n        text=re.sub(\"(mailto:)\", ' ', str(text)) #remove mailto:\n        text=re.sub(r\"(\\\\x9\\d)\", ' ', str(text)) #remove \\x9* in text\n        text=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)) #replace INC nums to INC_NUM\n        text=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(text)) #replace CM# and CHG# to CM_NUM\n\n        # url replacement into base form\n        try:\n            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n            repl_url = url.group(3)\n            text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(text))\n        except:\n            pass\n\n\n        # handle dot at the end of words\n        text=re.sub(\"(\\.\\s+)\", ' ', str(text))\n        \n        text=re.sub(\"(\\-\\s+)\", ' ', str(text)) #remove - at end of words(not between)\n        text=re.sub(\"(\\:\\s+)\", ' ', str(text)) #remove : at end of words(not between)\n\n        #remove multiple spaces\n        text=re.sub(\"(\\s+)\",' ',str(text))\n\n        # apply lowercase again\n        text = text.lower().strip()\n        \n        # remove trailing dot, we will apply end of sequence anyway\n        text = re.sub(\"(\\.)$\", '', str(text)).strip()\n\n        return text\n\n    def apply_special_tokens(self, text):\n        text = str(text).strip()\n        text = \"<sos> \" + str(text).strip() + \" <eos>\"\n        return text\n\n    def remove_special_tokens(self, text):\n        text = text.lower()\n        text = text.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n        text = text.replace(\"<unk>\", \"##\")\n        text = text.strip()\n        return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final summarization class using loadable TF graph model an processing\nclass Summarizer:\n    \n    # Initialize using SummarizationModule or loaded graph tf module\n    def __init__(self, module: tf.Module):\n        self.module = module\n        self.processor = TextProcessor()\n        \n    def summarize(self, text: str):\n        prepared = self.processor.apply_special_tokens(self.processor.clean_text(text))\n        output_text, output_tensor, weights = self.module.predict(tf.constant([prepared]))\n        return prepared, self.processor.remove_special_tokens(bytes.decode(output_text.numpy())), output_tensor, weights\n    \n    # Shorthand for text output only\n    def __call__(self, text: str):\n        return self.summarize(text)[1]\n    \nsummarizer = Summarizer(summarizer_module)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference on test and val sets","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Predict on test set and save\nprint('--- Runing inference on test set ---\\n')\ntest_pred = remove_special_tokens_frame(summarize_frame(df_test))\ntest_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred.to_csv('testset_evaluation_data.csv')\nprint('Saved test set evaluation data.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Predict on validation set with same size as test set and save\nprint('--- Runing inference on validation set ---\\n')\nval_pred = remove_special_tokens_frame(summarize_frame(df_val[:df_test.shape[0]]))\nval_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred.to_csv('validationset_evaluation_data.csv')\nprint('Saved validation set evaluation data.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('--- Example 10 test set summaries ---\\n')\npretty_summaries(test_pred[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('--- Example 10 validation set summaries ---\\n')\npretty_summaries(val_pred[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom example:","metadata":{}},{"cell_type":"code","source":"%%time\nsummarizer('An 18 year old man joe mama has been killed in a tragic car accident in washington us the police have reported that a car has crashed with a lorry this evening on new york street the injured man has died on site.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsummarizer('The us president Joe Biden has annouced that he will be coming to china in april to consult the chinese president xi jingping about the current nuclear threats.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"\n# note, will fail on empty seq\nbleu = evaluate.load('bleu')\nbleu_test = bleu.compute(references=test_pred['summary'], predictions=test_pred['predicted'])\nbleu_val = bleu.compute(references=val_pred['summary'], predictions=val_pred['predicted'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\nrouge_test = rouge.compute(references=test_pred['summary'], predictions=test_pred['predicted'])\nrouge_val = rouge.compute(references=val_pred['summary'], predictions=val_pred['predicted'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate metrics after inference and save to unified frame, save to \n\nev_metrics = {\n    'loss_best': min(history.history['loss']),\n    'masked_accuracy_best': max(history.history['masked_accuracy']),\n    'val_loss_best': min(history.history['val_loss']),\n    'val_masked_accuracy_best': max(history.history['val_masked_accuracy']),\n    \n    'test_rouge1': rouge_test['rouge1'],\n    'test_rouge2': rouge_test['rouge2'],\n    'test_rougeL': rouge_test['rougeL'],\n    'test_rougeLsum': rouge_test['rougeLsum'],\n    'test_bleu': bleu_test['bleu'],\n    \n    'val_rouge1': rouge_val['rouge1'],\n    'val_rouge2': rouge_val['rouge2'],\n    'val_rougeL': rouge_val['rougeL'],\n    'val_rougeLsum': rouge_val['rougeLsum'],\n    'val_bleu': bleu_val['bleu'],\n}\n\npd.DataFrame(ev_metrics, index=[0]).to_csv('evaluation_metrics.csv')\n\nprint(\"Evaluation metrics: \");\npprint(ev_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save to wandb metric summary\nfor metric in ev_metrics:\n    wandb_run.summary[metric] = ev_metrics[metric]\n\n\n# finish wandb run\nwandb_run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save summarization module","metadata":{}},{"cell_type":"code","source":"tf.saved_model.save(summarizer_module, 'summarizer_module')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_summarizer = Summarizer(tf.saved_model.load('summarizer_module'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_summarizer('London\\'s police force has failed to learn enough from its failures in a 2016 serial killer case to stop similar crimes happening again, a police watchdog said on Thursday in a damning report.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}