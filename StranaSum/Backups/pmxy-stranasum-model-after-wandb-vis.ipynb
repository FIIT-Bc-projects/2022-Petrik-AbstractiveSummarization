{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sebastian Petrik - Abstractive summarization transformer\n\nInspired by:\n- https://www.kaggle.com/code/ashishsingh226/text-summarization-with-transformers\n- https://www.tensorflow.org/text/tutorials/transformer\n","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade -q wandb --quiet\n!pip install evaluate rouge_score --quiet","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:07:22.659976Z","iopub.execute_input":"2023-03-25T00:07:22.660950Z","iopub.status.idle":"2023-03-25T00:07:49.086948Z","shell.execute_reply.started":"2023-03-25T00:07:22.660895Z","shell.execute_reply":"2023-03-25T00:07:49.085718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.environ.get('KAGGLE_CONTAINER_NAME')) # check if kaggle","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:07:49.089396Z","iopub.execute_input":"2023-03-25T00:07:49.089814Z","iopub.status.idle":"2023-03-25T00:07:49.097274Z","shell.execute_reply.started":"2023-03-25T00:07:49.089770Z","shell.execute_reply":"2023-03-25T00:07:49.096176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\nsorted(list(filter(\n    lambda x: x[0] in ['numpy', 'pandas', 'tensorflow', 'tensorflow-text', 'keras', 'tensorflow-estimator', 'tensorflow-datasets', 'wandb', 'evaluate', 'rouge_score'],\n    [(i.key, i.version) for i in pkg_resources.working_set]\n)))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:07:49.098960Z","iopub.execute_input":"2023-03-25T00:07:49.099722Z","iopub.status.idle":"2023-03-25T00:07:49.115562Z","shell.execute_reply.started":"2023-03-25T00:07:49.099685Z","shell.execute_reply":"2023-03-25T00:07:49.114511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nimport os\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport operator as op\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom pprint import pprint\nimport evaluate","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:07:49.119024Z","iopub.execute_input":"2023-03-25T00:07:49.119403Z","iopub.status.idle":"2023-03-25T00:08:03.337863Z","shell.execute_reply.started":"2023-03-25T00:07:49.119377Z","shell.execute_reply":"2023-03-25T00:08:03.336792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration and Wandb","metadata":{}},{"cell_type":"code","source":"# Setup seeds\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1' \nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:03.339444Z","iopub.execute_input":"2023-03-25T00:08:03.340253Z","iopub.status.idle":"2023-03-25T00:08:03.348786Z","shell.execute_reply.started":"2023-03-25T00:08:03.340220Z","shell.execute_reply":"2023-03-25T00:08:03.346347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    # Meta\n    wandb_project = 'stranasum-exploring',\n    wandb_group = '-',\n    host = 'kaggle',\n    \n    # Data\n    dataset_name = \"inshorts_nodot_10-70_3-16_v0.05_t0.05\",\n    val_split = 0.05,\n    test_split = 0.05,\n    \n    # Sequences\n    # - define lengths according to data\n    max_input_length = 70, # Encoder sequence length, max article len, max token count in E.\n    max_target_length = 16, # Decoder sequence length, max summary len, max token count in D.\n\n    # Transformer hyperparameters\n    num_layers = 3, # 4\n    d_model = 128, # 128 -> embedding length\n    dff = 512, # 512\n    num_heads = 8, # 8\n    dropout_rate = 0.1, # 0.1\n\n    # Training\n    early_stopping_patience = 3, # patience - num of non-improving consecutive epochs\n    max_epochs = 20, # 15\n    batch_size = 256, # 256\n    learning_rate_warmup_steps = 4000 # 4000\n)\n\nprint(\"Config:\")\npprint(CONFIG)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:03.350273Z","iopub.execute_input":"2023-03-25T00:08:03.351537Z","iopub.status.idle":"2023-03-25T00:08:03.363466Z","shell.execute_reply.started":"2023-03-25T00:08:03.351496Z","shell.execute_reply":"2023-03-25T00:08:03.362347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb.login(key=user_secrets.get_secret(\"wandb\"))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:03.365315Z","iopub.execute_input":"2023-03-25T00:08:03.365566Z","iopub.status.idle":"2023-03-25T00:08:05.982235Z","shell.execute_reply.started":"2023-03-25T00:08:03.365542Z","shell.execute_reply":"2023-03-25T00:08:05.981091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading","metadata":{}},{"cell_type":"markdown","source":"- load data from preprocessing","metadata":{}},{"cell_type":"code","source":"# show available data\n!ls ../input\n!ls ../input/pmxy-stranasum-preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:05.986702Z","iopub.execute_input":"2023-03-25T00:08:05.987459Z","iopub.status.idle":"2023-03-25T00:08:07.936753Z","shell.execute_reply.started":"2023-03-25T00:08:05.987414Z","shell.execute_reply":"2023-03-25T00:08:07.935459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir = \"../input/pmxy-stranasum-preprocessing\"\ndf_train = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_train.csv\")\ndf_val = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_val.csv\")\ndf_test = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_test.csv\")\n\nprint(\"Train:\", df_train.shape)\nprint(\"Val:\", df_val.shape)\nprint(\"Test:\", df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:07.938755Z","iopub.execute_input":"2023-03-25T00:08:07.939490Z","iopub.status.idle":"2023-03-25T00:08:08.577553Z","shell.execute_reply.started":"2023-03-25T00:08:07.939446Z","shell.execute_reply":"2023-03-25T00:08:08.576607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# Custom tokenizer\nclass Tokenizer(tf.Module):\n    \n    def __init__(self, vectorization_dataset: tf.data.Dataset, max_length: int):\n        super().__init__(name=\"Tokenizer\")\n        \n        self.max_length = max_length\n        \n        # Create and fit vectorizer from dataset\n        self.vectorizer = tf.keras.layers.TextVectorization(\n            output_mode='int',\n            output_sequence_length=self.max_length,\n            standardize=None\n        )\n        self.vectorizer.adapt(vectorization_dataset.batch(1024))\n        \n        # conversions\n        self.word_to_id = tf.keras.layers.StringLookup(\n            vocabulary=self.vectorizer.get_vocabulary(),\n            mask_token='', oov_token='[UNK]'\n        )\n        self.id_to_word = tf.keras.layers.StringLookup(\n            vocabulary=self.vectorizer.get_vocabulary(),\n            mask_token='', oov_token='[UNK]',\n            invert=True\n        )\n        \n        # attributes\n        self.vocab_size = self.vectorizer.vocabulary_size()\n        self.start_token = self.word_to_id('<sos>')\n        self.end_token = self.word_to_id('<eos>')\n        \n        print(f\"Tokenizer maxlen={self.max_length}, top vocabulary: {self.vectorizer.get_vocabulary()[:10]}\")\n        \n        \n    # to convert text to tokens, call vectorizer directly !\n        \n    # convert tokens back to text\n    @tf.function\n    def tokens_to_text(self, tokens):\n        words = self.id_to_word(tokens)\n        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n        result = tf.strings.regex_replace(result, '^ *<sos> *', '')\n        result = tf.strings.regex_replace(result, ' *<eos> *$', '')\n        result = tf.strings.regex_replace(result, '<dot>', '.')\n        return result\n\n# Setup tokenizers\n\ninput_tokenizer = Tokenizer(\n    tf.data.Dataset.from_tensor_slices(df_train['article']),\n    CONFIG['max_input_length']\n)\n\ntarget_tokenizer = Tokenizer(\n    tf.data.Dataset.from_tensor_slices(df_train['summary']),\n    CONFIG['max_target_length']\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:08.582409Z","iopub.execute_input":"2023-03-25T00:08:08.582715Z","iopub.status.idle":"2023-03-25T00:08:19.074917Z","shell.execute_reply.started":"2023-03-25T00:08:08.582685Z","shell.execute_reply":"2023-03-25T00:08:19.073733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try out \nsample_tokens = input_tokenizer.vectorizer(tf.constant(['<sos> the dog ate the food <eos>']))\nsample_tokens","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:19.076712Z","iopub.execute_input":"2023-03-25T00:08:19.077458Z","iopub.status.idle":"2023-03-25T00:08:19.175685Z","shell.execute_reply.started":"2023-03-25T00:08:19.077416Z","shell.execute_reply":"2023-03-25T00:08:19.174585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenizer.tokens_to_text(sample_tokens).numpy().astype('str') # binary string tensor into decoded string array","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:19.177045Z","iopub.execute_input":"2023-03-25T00:08:19.180117Z","iopub.status.idle":"2023-03-25T00:08:19.393695Z","shell.execute_reply.started":"2023-03-25T00:08:19.180087Z","shell.execute_reply":"2023-03-25T00:08:19.392595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_tokenizer.vectorizer(tf.constant('<sos> a man was murdered <eos>'))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:19.395077Z","iopub.execute_input":"2023-03-25T00:08:19.395536Z","iopub.status.idle":"2023-03-25T00:08:19.436715Z","shell.execute_reply.started":"2023-03-25T00:08:19.395495Z","shell.execute_reply":"2023-03-25T00:08:19.435699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_tokenized_sequences(input_texts: np.array, target_texts: np.array):\n    \n    # convert numpy text arrays into vectors\n    inputs = input_tokenizer.vectorizer(input_texts)\n    targets = target_tokenizer.vectorizer(target_texts)\n    \n    # drop EOS token\n    targets_inputs = targets[:,:-1]\n    \n    # drop SOS token, shifting sequence 1 step behind providing next word labels for each step\n    targets_labels = targets[:,1:]\n    \n    return (inputs, targets_inputs), targets_labels","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:19.438028Z","iopub.execute_input":"2023-03-25T00:08:19.438692Z","iopub.status.idle":"2023-03-25T00:08:19.445540Z","shell.execute_reply.started":"2023-03-25T00:08:19.438634Z","shell.execute_reply":"2023-03-25T00:08:19.444424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply tokenization and create batched tf datasets ...\n\n# convert to sequences and then into a tensorflow dataset\n# shuffle - shuffle with buffer size = size of data for full uniform shuffle\n\ndataset_train = tf.data.Dataset.from_tensor_slices(\n    convert_to_tokenized_sequences(df_train['article'], df_train['summary'])\n).shuffle(df_train.shape[0]).batch(CONFIG['batch_size'])\n\ndataset_val = tf.data.Dataset.from_tensor_slices(\n    convert_to_tokenized_sequences(df_val['article'], df_val['summary'])\n).shuffle(df_val.shape[0]).batch(CONFIG['batch_size'])\n\n# take 1 sample batch for further inspection (later)\nfor (sample_input, sample_target), sample_target_labels in dataset_train.take(1):\n    break\n    \nprint('Sample single batch from dataset:')\n\nprint(sample_input.shape)\nprint(sample_target.shape)\nprint(sample_target_labels.shape)\n\nprint(sample_target[0])\nprint(sample_target_labels[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:19.447390Z","iopub.execute_input":"2023-03-25T00:08:19.447753Z","iopub.status.idle":"2023-03-25T00:08:20.297830Z","shell.execute_reply.started":"2023-03-25T00:08:19.447718Z","shell.execute_reply":"2023-03-25T00:08:20.296832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try to get special token numbers so we can see them in prints\nprint(input_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> <dot> <pad>')))\nprint(target_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> <dot> <pad>')))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:20.299438Z","iopub.execute_input":"2023-03-25T00:08:20.299796Z","iopub.status.idle":"2023-03-25T00:08:20.330134Z","shell.execute_reply.started":"2023-03-25T00:08:20.299759Z","shell.execute_reply":"2023-03-25T00:08:20.329228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer model implementation","metadata":{}},{"cell_type":"code","source":"def positional_encoding(length, depth):\n    depth = depth/2\n\n    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n\n    angle_rates = 1 / (10000**depths)         # (1, depth)\n    angle_rads = positions * angle_rates      # (pos, depth)\n\n    pos_encoding = np.concatenate(\n      [np.sin(angle_rads), np.cos(angle_rads)],\n      axis=-1) \n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n# We try out the encoding func\npos_encoding = positional_encoding(length=2048, depth=512)\nprint('Positional encoding shape', pos_encoding.shape) # Check the shape.\n\n# Plot the dimensions.\nplt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:20.331514Z","iopub.execute_input":"2023-03-25T00:08:20.331887Z","iopub.status.idle":"2023-03-25T00:08:21.125423Z","shell.execute_reply.started":"2023-03-25T00:08:20.331850Z","shell.execute_reply":"2023-03-25T00:08:21.124342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, d_model):\n        super().__init__()\n        self.d_model = d_model\n        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n    def compute_mask(self, *args, **kwargs):\n        return self.embedding.compute_mask(*args, **kwargs)\n\n    def call(self, x):\n        length = tf.shape(x)[1]\n        x = self.embedding(x)\n        # This factor sets the relative scale of the embedding and positonal_encoding.\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x = x + self.pos_encoding[tf.newaxis, :length, :]\n        return x \n\n# Try\n# sample_emb_input = PositionalEmbedding(vocab_size=input_tokenizer.vocab_size, d_model=512)(sample_input)\n# sample_emb_target = PositionalEmbedding(vocab_size=target_tokenizer.vocab_size, d_model=512)(sample_target)\n\n# sample_emb_target._keras_mask","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.126536Z","iopub.execute_input":"2023-03-25T00:08:21.127113Z","iopub.status.idle":"2023-03-25T00:08:21.137733Z","shell.execute_reply.started":"2023-03-25T00:08:21.127074Z","shell.execute_reply":"2023-03-25T00:08:21.135429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base attention for further subclassing\nclass BaseAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n        self.layernorm = tf.keras.layers.LayerNormalization()\n        self.add = tf.keras.layers.Add()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.139561Z","iopub.execute_input":"2023-03-25T00:08:21.140354Z","iopub.status.idle":"2023-03-25T00:08:21.148445Z","shell.execute_reply.started":"2023-03-25T00:08:21.140314Z","shell.execute_reply":"2023-03-25T00:08:21.147419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrossAttention(BaseAttention):\n    \n    # x = target sequence, context = context sequence\n    def call(self, x, context):\n        attn_output, attn_scores = self.mha(\n            query=x,\n            key=context,\n            value=context,\n            return_attention_scores=True)\n\n        # Cache the attention scores for plotting later.\n        self.last_attn_scores = attn_scores\n\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n\n        return x\n\n# Try on sample\n# sample_ca = CrossAttention(num_heads=2, key_dim=512)\n# print(sample_emb_input.shape)\n# print(sample_emb_target.shape)\n# print(sample_ca(sample_emb_input, sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.150461Z","iopub.execute_input":"2023-03-25T00:08:21.150981Z","iopub.status.idle":"2023-03-25T00:08:21.163286Z","shell.execute_reply.started":"2023-03-25T00:08:21.150943Z","shell.execute_reply":"2023-03-25T00:08:21.161955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GlobalSelfAttention(BaseAttention):\n    def call(self, x):\n        attn_output = self.mha(\n            query=x,\n            value=x,\n            key=x)\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n        return x\n\n# Try\n# sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n# print(sample_emb_input.shape)\n# print(sample_gsa(sample_emb_input).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.165439Z","iopub.execute_input":"2023-03-25T00:08:21.166040Z","iopub.status.idle":"2023-03-25T00:08:21.173617Z","shell.execute_reply.started":"2023-03-25T00:08:21.165998Z","shell.execute_reply":"2023-03-25T00:08:21.172193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(BaseAttention):\n    def call(self, x):\n        attn_output = self.mha(\n            query=x,\n            value=x,\n            key=x,\n            use_causal_mask = True)\n        x = self.add([x, attn_output])\n        x = self.layernorm(x)\n        return x\n    \n# sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n# print(sample_emb_target.shape)\n# print(sample_csa(sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.175525Z","iopub.execute_input":"2023-03-25T00:08:21.175981Z","iopub.status.idle":"2023-03-25T00:08:21.184432Z","shell.execute_reply.started":"2023-03-25T00:08:21.175943Z","shell.execute_reply":"2023-03-25T00:08:21.183355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test latter elements do not depend on earlier elements, making no difference\n# if we remove the earlier elements before or after applying csa layer\n# out1 = sample_csa(embed_target(sample_target[:, :3])) \n# out2 = sample_csa(embed_target(sample_target))[:, :3]\n# tf.reduce_max(abs(out1 - out2)).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.186247Z","iopub.execute_input":"2023-03-25T00:08:21.186787Z","iopub.status.idle":"2023-03-25T00:08:21.193714Z","shell.execute_reply.started":"2023-03-25T00:08:21.186747Z","shell.execute_reply":"2023-03-25T00:08:21.192514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(tf.keras.layers.Layer):\n    def __init__(self, d_model, dff, dropout_rate=0.1):\n        super().__init__()\n        self.seq = tf.keras.Sequential([\n          tf.keras.layers.Dense(dff, activation='relu'),\n          tf.keras.layers.Dense(d_model),\n          tf.keras.layers.Dropout(dropout_rate)\n        ])\n        self.add = tf.keras.layers.Add()\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n\n    def call(self, x):\n        x = self.add([x, self.seq(x)])\n        x = self.layer_norm(x) \n        return x\n\n# Try\n# sample_ffn = FeedForward(512, 2048)\n# print(sample_emb_target.shape)\n# print(sample_ffn(sample_emb_target).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.195748Z","iopub.execute_input":"2023-03-25T00:08:21.196159Z","iopub.status.idle":"2023-03-25T00:08:21.206988Z","shell.execute_reply.started":"2023-03-25T00:08:21.196121Z","shell.execute_reply":"2023-03-25T00:08:21.205874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n        super().__init__()\n\n        self.self_attention = GlobalSelfAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.ffn = FeedForward(d_model, dff)\n\n    def call(self, x):\n        x = self.self_attention(x)\n        x = self.ffn(x)\n        return x\n    \n# sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n# print(sample_encoder_layer(sample_emb_input).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.208765Z","iopub.execute_input":"2023-03-25T00:08:21.209255Z","iopub.status.idle":"2023-03-25T00:08:21.216942Z","shell.execute_reply.started":"2023-03-25T00:08:21.209218Z","shell.execute_reply":"2023-03-25T00:08:21.215782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, *, num_layers, d_model, num_heads,\n                   dff, vocab_size, dropout_rate=0.1):\n        \n        super().__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.pos_embedding = PositionalEmbedding(\n            vocab_size=vocab_size, d_model=d_model)\n\n        self.enc_layers = [\n            EncoderLayer(d_model=d_model,\n                         num_heads=num_heads,\n                         dff=dff,\n                         dropout_rate=dropout_rate)\n            for _ in range(num_layers)\n        ]\n        \n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, x):\n        # `x` is token-IDs shape: (batch_size, seq_len)\n        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n\n        # Add dropout.\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x)\n\n        return x  # Shape `(batch_size, seq_len, d_model)`.\n\n# Try\n# sample_encoder = Encoder(num_layers=4,\n#                          d_model=512,\n#                          num_heads=8,\n#                          dff=2048,\n#                          vocab_size=input_tokenizer.vocab_size)\n\n# sample_encoder_output = sample_encoder(sample_input, training=False)\n\n# print(sample_input.shape)\n# print(sample_encoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.218962Z","iopub.execute_input":"2023-03-25T00:08:21.219358Z","iopub.status.idle":"2023-03-25T00:08:21.228026Z","shell.execute_reply.started":"2023-03-25T00:08:21.219324Z","shell.execute_reply":"2023-03-25T00:08:21.226903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    \n    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.causal_self_attention = CausalSelfAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.cross_attention = CrossAttention(\n            num_heads=num_heads,\n            key_dim=d_model,\n            dropout=dropout_rate)\n\n        self.ffn = FeedForward(d_model, dff)\n\n    def call(self, x, context):\n        x = self.causal_self_attention(x=x)\n        x = self.cross_attention(x=x, context=context)\n\n        # Cache the last attention scores for plotting later\n        self.last_attn_scores = self.cross_attention.last_attn_scores\n\n        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n        return x\n    \n# sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n# sample_decoder_layer_output = sample_decoder_layer(\n#     x=sample_emb_target, context=sample_emb_input\n# )\n\n# print(sample_emb_input.shape)\n# print(sample_emb_target.shape)\n# print(sample_decoder_layer_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.229840Z","iopub.execute_input":"2023-03-25T00:08:21.230184Z","iopub.status.idle":"2023-03-25T00:08:21.241368Z","shell.execute_reply.started":"2023-03-25T00:08:21.230150Z","shell.execute_reply":"2023-03-25T00:08:21.240289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n               dropout_rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n                                                 d_model=d_model)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.dec_layers = [\n            DecoderLayer(d_model=d_model, num_heads=num_heads,\n                         dff=dff, dropout_rate=dropout_rate)\n            for _ in range(num_layers)]\n\n        self.last_attn_scores = None\n\n    def call(self, x, context):\n        # `x` is token-IDs shape (batch, target_seq_len)\n        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x  = self.dec_layers[i](x, context)\n\n        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n\n        # The shape of x is (batch_size, target_seq_len, d_model).\n        return x\n\n# Try\n# sample_decoder = Decoder(num_layers=4,\n#                          d_model=512,\n#                          num_heads=8,\n#                          dff=2048,\n#                          vocab_size=target_tokenizer.vocab_size)\n\n# sample_decoder_output = sample_decoder(x=sample_target, context=sample_emb_input)\n\n# print(sample_target.shape)\n# print(sample_emb_input.shape)\n# print(sample_decoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.248955Z","iopub.execute_input":"2023-03-25T00:08:21.249229Z","iopub.status.idle":"2023-03-25T00:08:21.257992Z","shell.execute_reply.started":"2023-03-25T00:08:21.249204Z","shell.execute_reply":"2023-03-25T00:08:21.256821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_decoder.last_attn_scores.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.259744Z","iopub.execute_input":"2023-03-25T00:08:21.260146Z","iopub.status.idle":"2023-03-25T00:08:21.271393Z","shell.execute_reply.started":"2023-03-25T00:08:21.260110Z","shell.execute_reply":"2023-03-25T00:08:21.270372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n        \n        super().__init__()\n        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               vocab_size=input_vocab_size,\n                               dropout_rate=dropout_rate)\n\n        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               vocab_size=target_vocab_size,\n                               dropout_rate=dropout_rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inputs):\n        # To use a Keras model with `.fit` you must pass all your inputs in the\n        # first argument.\n        context, x  = inputs\n\n        context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n        # Final linear layer output.\n        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n        try:\n            # Drop the keras mask, so it doesn't scale the losses/metrics.\n            # b/250038731\n            del logits._keras_mask\n        except AttributeError:\n            pass\n\n        # Return the final output and the attention weights.\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.273122Z","iopub.execute_input":"2023-03-25T00:08:21.273508Z","iopub.status.idle":"2023-03-25T00:08:21.283409Z","shell.execute_reply.started":"2023-03-25T00:08:21.273472Z","shell.execute_reply":"2023-03-25T00:08:21.282399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct transformer\ntransformer = Transformer(\n    num_layers=CONFIG['num_layers'],\n    d_model=CONFIG['d_model'],\n    num_heads=CONFIG['num_heads'],\n    dff=CONFIG['dff'],\n    input_vocab_size=input_tokenizer.vocab_size,\n    target_vocab_size=target_tokenizer.vocab_size,\n    dropout_rate=CONFIG['dropout_rate']\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.286583Z","iopub.execute_input":"2023-03-25T00:08:21.286945Z","iopub.status.idle":"2023-03-25T00:08:21.404921Z","shell.execute_reply.started":"2023-03-25T00:08:21.286918Z","shell.execute_reply":"2023-03-25T00:08:21.403877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenizer.vocab_size, target_tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.406605Z","iopub.execute_input":"2023-03-25T00:08:21.406991Z","iopub.status.idle":"2023-03-25T00:08:21.413983Z","shell.execute_reply.started":"2023-03-25T00:08:21.406951Z","shell.execute_reply":"2023-03-25T00:08:21.412878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call transformer on sample input, this will build it and setup inputs\n\nsample_transformer_output = transformer((sample_input, sample_target))\nprint(sample_input.shape)\nprint(sample_target.shape)\nprint(sample_transformer_output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:21.415873Z","iopub.execute_input":"2023-03-25T00:08:21.416775Z","iopub.status.idle":"2023-03-25T00:08:25.264673Z","shell.execute_reply.started":"2023-03-25T00:08:21.416735Z","shell.execute_reply":"2023-03-25T00:08:25.263546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_transformer_attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n# print(sample_transformer_attn_scores.shape) ","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:25.266017Z","iopub.execute_input":"2023-03-25T00:08:25.267139Z","iopub.status.idle":"2023-03-25T00:08:25.271827Z","shell.execute_reply.started":"2023-03-25T00:08:25.267088Z","shell.execute_reply":"2023-03-25T00:08:25.270505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:25.273368Z","iopub.execute_input":"2023-03-25T00:08:25.274538Z","iopub.status.idle":"2023-03-25T00:08:25.315767Z","shell.execute_reply.started":"2023-03-25T00:08:25.274500Z","shell.execute_reply":"2023-03-25T00:08:25.314719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps):\n        super().__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \n    def get_config(self):\n        config = {\n            'd_model': self.d_model,\n            'warmup_steps': self.warmup_steps\n        }\n        return config\n#         base_config = super(CustomSchedule, self).get_config()\n#         return dict(list(base_config.items()) + list(config.items()))\n\nlearning_rate = CustomSchedule(CONFIG['d_model'], CONFIG['learning_rate_warmup_steps'])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\n\nplt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\nplt.ylabel('Learning Rate')\nplt.xlabel('Train Step')","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:25.317126Z","iopub.execute_input":"2023-03-25T00:08:25.318163Z","iopub.status.idle":"2023-03-25T00:08:25.586242Z","shell.execute_reply.started":"2023-03-25T00:08:25.318124Z","shell.execute_reply":"2023-03-25T00:08:25.585193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_loss(label, pred):\n    mask = label != 0\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none'\n    )\n    loss = loss_object(label, pred)\n\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss *= mask\n\n    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n    \n    return loss\n\n\ndef masked_accuracy(label, pred):\n    pred = tf.argmax(pred, axis=2)\n    label = tf.cast(label, pred.dtype)\n    match = label == pred\n\n    mask = label != 0\n\n    match = match & mask\n\n    match = tf.cast(match, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n\n\n# masked_loss(tf.constant([0.5], dtype=tf.float32), tf.constant([[0.5, 0.2]], dtype=tf.float32))","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:25.587637Z","iopub.execute_input":"2023-03-25T00:08:25.588687Z","iopub.status.idle":"2023-03-25T00:08:25.597745Z","shell.execute_reply.started":"2023-03-25T00:08:25.588617Z","shell.execute_reply":"2023-03-25T00:08:25.596497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:08:25.599445Z","iopub.execute_input":"2023-03-25T00:08:25.600255Z","iopub.status.idle":"2023-03-25T00:08:25.623016Z","shell.execute_reply.started":"2023-03-25T00:08:25.600216Z","shell.execute_reply":"2023-03-25T00:08:25.621803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Callbacks \n!mkdir -p checkpoints\n\nmodeldir = f\"checkpoints\"\ncheckpoint_filepath = modeldir + '/checkpoint.hdf'\nprint('Model checkpoint:', checkpoint_filepath)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode='min', verbose=1,\n    patience=CONFIG['early_stopping_patience'],\n    restore_best_weights=True # restore only best weights relative to val_loss\n)\n\ncsv_logger=tf.keras.callbacks.CSVLogger(\n    modeldir + '/log.csv', separator=\",\", append=True\n)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True\n)\n\ntry:\n    model.load_weights(checkpoint_filepath)\n    print('Loaded model weights checkpoint.')\nexcept:\n    print('Cannot load model weights from checkpoint, it may not exist yet.')","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:10:50.304072Z","iopub.execute_input":"2023-03-25T00:10:50.304466Z","iopub.status.idle":"2023-03-25T00:10:51.310622Z","shell.execute_reply.started":"2023-03-25T00:10:50.304433Z","shell.execute_reply":"2023-03-25T00:10:51.309239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Training\n\ndo_train = True\n\nhistory = None\nif do_train:\n    \n    run = wandb.init(\n        project=CONFIG['wandb_project'], \n        config=CONFIG,\n        group=CONFIG['wandb_group'], \n        job_type='train'\n    )\n    \n    history = transformer.fit(\n        dataset_train,\n        epochs=CONFIG['max_epochs'],\n        validation_data=dataset_val,\n        callbacks=[\n            WandbCallback(save_model=False),\n            early_stopping,\n            csv_logger,\n            model_checkpoint_callback\n        ]\n    )\n    \n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:10:51.312994Z","iopub.execute_input":"2023-03-25T00:10:51.313698Z","iopub.status.idle":"2023-03-25T00:11:27.224550Z","shell.execute_reply.started":"2023-03-25T00:10:51.313630Z","shell.execute_reply":"2023-03-25T00:11:27.222279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    fig, (axl, axa) = plt.subplots(nrows=2, ncols=1)\n    axl.plot(history.history['loss'], label='loss')\n    axl.plot(history.history['val_loss'], label='val_loss')\n    axl.set_ylim([0, 10])\n    axl.set_xlabel('Epoch')\n    axl.set_ylabel('Loss')\n    axl.legend()\n    axl.grid(True)\n    \n    axa.plot(history.history['masked_accuracy'], label='masked_accuracy')\n    axa.plot(history.history['val_masked_accuracy'], label='val_masked_accuracy')\n    axa.set_ylim([0, 1])\n    axa.set_xlabel('Epoch')\n    axa.set_ylabel('Accuracy')\n    axa.legend()\n    axa.grid(True)\n    \n    fig.show()\n    \nif history != None:\n    plot_history(history)\nelse:\n    print(\"No history to display.\")","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.226348Z","iopub.execute_input":"2023-03-25T00:11:27.229172Z","iopub.status.idle":"2023-03-25T00:11:27.244497Z","shell.execute_reply.started":"2023-03-25T00:11:27.229122Z","shell.execute_reply":"2023-03-25T00:11:27.243024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# LEGACY - unoptimized inference using python array\nclass SummarizerSingleStep(tf.Module):\n    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n        \n        # todo\n        self.transformer = transformer\n        self.input_tokenizer = input_tokenizer\n        self.target_tokenizer = target_tokenizer\n\n    # expect sentence to be prepared with <sos> <eos> and clean\n    # @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n    def __call__(self, sentence: str):\n\n        encoder_input = tf.expand_dims(self.input_tokenizer.vectorizer(sentence), 0)\n\n        # `tf.TensorArray` is required here (instead of a Python list), so that the\n        # dynamic-loop can be traced by `tf.function`.\n        # output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        # output_array = output_array.write(0, summary_start_token)\n        \n        summary_start_token = target_tokenizer.vectorizer('<sos>')[0].numpy()\n        summary_end_token = target_tokenizer.vectorizer('<eos>')[0].numpy()\n        \n        decoder_input = [summary_start_token]\n        output = tf.expand_dims(decoder_input, 0)\n\n        for i in tf.range(self.target_tokenizer.max_length):\n            \n            predictions = self.transformer([encoder_input, output], training=False)\n\n            # Select the last token from the `seq_len` dimension.\n            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n            # TODO: why cast needed here ??\n            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), dtype=tf.int32)\n\n            # Concatenate the `predicted_id` to the output which is given to the\n            # decoder as its input.\n            # output_array = output_array.write(i+1, predicted_id[0])\n            output = tf.concat([output, predicted_id], axis=-1)\n\n            if predicted_id == summary_end_token:\n                break\n        \n        \n        prediction = tf.squeeze(output, axis=0)\n        tokens = np.expand_dims(prediction.numpy(), 0)\n        \n        # print(tokens)\n        \n        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n        text = self.target_tokenizer.tokens_to_text(tokens)[0]\n\n        # `tf.function` prevents us from using the attention_weights that were\n        # calculated on the last iteration of the loop.\n        # So, recalculate them outside the loop.\n        self.transformer([encoder_input, output[:,:-1]], training=False)\n        attention_weights = self.transformer.decoder.last_attn_scores\n\n        return text, tokens, attention_weights\n\n# Optimized inference using tensorflow tensorarray\nclass Summarizer(tf.Module):\n    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n        \n        # todo\n        self.transformer = transformer\n        self.input_tokenizer = input_tokenizer\n        self.target_tokenizer = target_tokenizer\n        \n    def to_tf(self, text: str):\n        return tf.constant([text])\n    \n    def from_tf(self, tensor: tf.Tensor):\n        return bytes.decode(tensor.numpy())\n\n    # expect sentence to be prepared with <sos> <eos> and clean\n    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n    def __call__(self, sentence: tf.Tensor):\n\n        encoder_input = self.input_tokenizer.vectorizer(sentence)\n        \n        start_token = target_tokenizer.vectorizer(tf.constant('<sos>'))[0][tf.newaxis]\n        end_token = target_tokenizer.vectorizer(tf.constant('<eos>'))[0][tf.newaxis]\n        \n        # `tf.TensorArray` is required here (instead of a Python list), so that the\n        # dynamic-loop can be traced by `tf.function`.\n        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        output_array = output_array.write(0, start_token)\n        output = tf.transpose(output_array.stack())\n\n        for i in tf.range(self.target_tokenizer.max_length):\n            \n            output = tf.transpose(output_array.stack())\n            \n            predictions = self.transformer([encoder_input, output], training=False)\n\n            # Select the last token from the `seq_len` dimension.\n            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n            # argmax\n            predicted_id = tf.argmax(predictions, axis=-1)\n\n            # Concatenate the `predicted_id` to the output which is given to the\n            # decoder as its input.\n            # output_array = output_array.write(i+1, predicted_id[0])\n            output_array = output_array.write(i+1, predicted_id[0])\n\n            # stop on end\n            if predicted_id == end_token:\n                break\n        \n        output = tf.transpose(output_array.stack())\n        \n        # print(tokens)\n        \n        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n        text = self.target_tokenizer.tokens_to_text(output)[0]\n\n        # `tf.function` prevents us from using the attention_weights that were\n        # calculated on the last iteration of the loop.\n        # So, recalculate them outside the loop.\n        self.transformer([encoder_input, output[:,:-1]], training=False)\n        attention_weights = self.transformer.decoder.last_attn_scores\n\n        return text, output, attention_weights\n\n\nsummarizer = Summarizer(transformer, input_tokenizer, target_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.252760Z","iopub.execute_input":"2023-03-25T00:11:27.253613Z","iopub.status.idle":"2023-03-25T00:11:27.282702Z","shell.execute_reply.started":"2023-03-25T00:11:27.253574Z","shell.execute_reply":"2023-03-25T00:11:27.281600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# (inference benchmark)\n\n# summarizer_single = SummarizerSingleStep(transformer, input_tokenizer, target_tokenizer)\n# df_train[:5]['article'].apply(lambda text: summarizer_single(text)[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.286478Z","iopub.execute_input":"2023-03-25T00:11:27.287051Z","iopub.status.idle":"2023-03-25T00:11:27.300318Z","shell.execute_reply.started":"2023-03-25T00:11:27.287023Z","shell.execute_reply":"2023-03-25T00:11:27.299191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# df_train[:5]['article'].apply(lambda text: summarizer(summarizer.to_tf(text))[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.301833Z","iopub.execute_input":"2023-03-25T00:11:27.302490Z","iopub.status.idle":"2023-03-25T00:11:27.319069Z","shell.execute_reply.started":"2023-03-25T00:11:27.302400Z","shell.execute_reply":"2023-03-25T00:11:27.317908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_tokens(text):\n        text = text.lower()\n        text = text.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n        text = text.replace(\"<unk>\", \"##\")\n        text = text.replace(\"<dot>\", \". \") # normal syntax with dot at end\n        text = text.strip()\n        return text\n\ndef remove_special_tokens_frame(frame: pd.DataFrame):\n    frame['article'] = frame['article'].apply(remove_special_tokens)\n    frame['summary'] = frame['summary'].apply(remove_special_tokens)\n    frame['predicted'] = frame['predicted'].apply(remove_special_tokens)\n    return frame\n    \n# Run summarization inference on entire frame\ndef summarize_frame(frame):\n    \n    frame = frame.copy()\n    frame['predicted'] = '<NONE>'\n    \n    for i in range(0, frame.shape[0]):\n        if i%25 == 0:\n            print(f\"Summarising ... {i}/{frame.shape[0]}\")\n            \n        article = frame.iloc[i]['article']\n        summary = frame.iloc[i]['summary']\n        \n        summarized_tf, summarized_tokens, attention_weights = summarizer(\n            tf.constant([article])\n        )\n        \n        summarized_text = summarizer.from_tf(summarized_tf)\n        \n        frame.iloc[i, frame.columns.get_loc('predicted')] = summarized_text\n        \n    return frame\n\ndef pretty_summaries(frame):\n    \n    for i, row in frame.iterrows():\n        print(f\"\\n ------------------\")\n        print(f\"Article  : {remove_special_tokens(row['article'])}\")\n        print(f\"\\nSummary  : {remove_special_tokens(row['summary'])}\")\n        print(f\"\\nPredicted: {remove_special_tokens(row['predicted'])}\")\n        print()\n        print(f\"------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.320230Z","iopub.execute_input":"2023-03-25T00:11:27.321448Z","iopub.status.idle":"2023-03-25T00:11:27.336764Z","shell.execute_reply.started":"2023-03-25T00:11:27.321419Z","shell.execute_reply":"2023-03-25T00:11:27.335491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference on test and val sets","metadata":{}},{"cell_type":"code","source":"wandb_eval_run = wandb.init(\n    project=CONFIG['wandb_project'], \n    config=CONFIG,\n    group=CONFIG['wandb_group'], \n    job_type='evaluate'\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:11:27.338446Z","iopub.execute_input":"2023-03-25T00:11:27.339532Z","iopub.status.idle":"2023-03-25T00:12:02.878383Z","shell.execute_reply.started":"2023-03-25T00:11:27.339470Z","shell.execute_reply":"2023-03-25T00:12:02.877438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Predict on test set and save\nprint('--- Runing inference on test set ---\\n')\ntest_pred = remove_special_tokens_frame(summarize_frame(df_test[:10]))\ntest_pred","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:17:56.816575Z","iopub.execute_input":"2023-03-25T00:17:56.817888Z","iopub.status.idle":"2023-03-25T00:17:58.649022Z","shell.execute_reply.started":"2023-03-25T00:17:56.817835Z","shell.execute_reply":"2023-03-25T00:17:58.647921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred.to_csv('testset_evaluation_data.csv')\nprint('Saved test set evaluation data.')","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:17:58.651317Z","iopub.execute_input":"2023-03-25T00:17:58.652081Z","iopub.status.idle":"2023-03-25T00:17:58.661711Z","shell.execute_reply.started":"2023-03-25T00:17:58.652042Z","shell.execute_reply":"2023-03-25T00:17:58.660488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Predict on test set and save\nprint('--- Runing inference on validation set ---\\n')\nval_pred = remove_special_tokens_frame(summarize_frame(df_val[:10]))\nval_pred","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:17:58.663575Z","iopub.execute_input":"2023-03-25T00:17:58.664421Z","iopub.status.idle":"2023-03-25T00:18:00.447060Z","shell.execute_reply.started":"2023-03-25T00:17:58.664382Z","shell.execute_reply":"2023-03-25T00:18:00.445972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred.to_csv('validationset_evaluation_data.csv')\nprint('Saved validation set evaluation data.')","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:18:00.449786Z","iopub.execute_input":"2023-03-25T00:18:00.450759Z","iopub.status.idle":"2023-03-25T00:18:00.459234Z","shell.execute_reply.started":"2023-03-25T00:18:00.450718Z","shell.execute_reply":"2023-03-25T00:18:00.457931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('--- Example test set summaries ---\\n')\npretty_summaries(test_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:18:00.460795Z","iopub.execute_input":"2023-03-25T00:18:00.461334Z","iopub.status.idle":"2023-03-25T00:18:00.485374Z","shell.execute_reply.started":"2023-03-25T00:18:00.461298Z","shell.execute_reply":"2023-03-25T00:18:00.482315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('--- Example validation set summaries ---\\n')\npretty_summaries(val_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:18:00.486821Z","iopub.execute_input":"2023-03-25T00:18:00.488135Z","iopub.status.idle":"2023-03-25T00:18:00.511703Z","shell.execute_reply.started":"2023-03-25T00:18:00.488088Z","shell.execute_reply":"2023-03-25T00:18:00.510695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Immediate metrics after inference\n\nrouge = evaluate.load('rouge')\nrouge_test = rouge.compute(references=test_pred['summary'], predictions=test_pred['predicted'])\nrouge_val = rouge.compute(references=val_pred['summary'], predictions=val_pred['predicted'])\n\nwandb_eval_run.log({\n    'rouge_metrics': wandb.Table(dataframe=pd.DataFrame({\n        'test': pd.Series(rouge_test),\n        'val': pd.Series(rouge_val)\n    })),\n})\n\nwandb_eval_run.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-25T00:22:47.784428Z","iopub.execute_input":"2023-03-25T00:22:47.784832Z","iopub.status.idle":"2023-03-25T00:22:53.139813Z","shell.execute_reply.started":"2023-03-25T00:22:47.784796Z","shell.execute_reply":"2023-03-25T00:22:53.138827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}